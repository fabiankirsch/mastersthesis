{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\cleardoublepage\n",
    "\n",
    "\n",
    "## LSTM - activities only\n",
    "\n",
    "An LSTM network with 1 or more LSTM layers and a Dense layer as the output layer is implemented. Assuming that activities are easier to model than transitions only sequences that are labeled as activities are considered in this section and transitions are removed. Weights are optimized using the ADAM optimizer [@kingma_adam:_2014], which needs relatively little computational resources. For the loss function categorical cross-entropy is used, which generally performs best for multi-classification problems. See @lst:lstm_base_cfg for the base configuration used for the LSTM. This first configuration was conceived based on experience, but has not been tested on this data.\n",
    "\n",
    "```{#lst:lstm_base_cfg caption='Base configuration for LSTM' .yaml}\n",
    "\n",
    "lstm_layer:\n",
    "  units: 100\n",
    "  activation: relu\n",
    "  dropout: 0.2\n",
    "  kernel_regularizer_l2: 0\n",
    "  activity_regularizer_l1: 0\n",
    "output_layer:\n",
    "  activation: softmax\n",
    "  kernel_regularizer_l2: 0\n",
    "  activity_regularizer_l1: 0\n",
    "loss: categorical_crossentropy\n",
    "optimizer:\n",
    "  adam:\n",
    "    lr: 0.001\n",
    "    beta_1: 0.9\n",
    "    beta_2: 0.999\n",
    "    decay: 0.0\n",
    "metrics: ['accuracy']\n",
    "epochs: 100\n",
    "batch_size: 200\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": "masterthesis_tf_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
